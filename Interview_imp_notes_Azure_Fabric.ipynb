{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Intro**"
      ],
      "metadata": {
        "id": "f6XQhDON7vWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hello, I am Rakesh.\n",
        "\n",
        "I have 7+ years of experience in IT, out of which 4+ years are focused specifically on Azure and 1 year on Microsoft Fabric, working across diverse domains like Banking, Retail, Finance, and Automotive.\n",
        "\n",
        "In my most recent project with ALD Automotive, my responsibility is to design, develop, and optimize data pipelines using the Medallion Architecture on both Azure and Fabric platforms.\n",
        "I work on ingesting data from various upstream source systems like SQL Server, SFTP, and ADLS Gen2 into the Bronze Layer.\n",
        "In the Silver Layer, I handle data preprocessing, quality checks, standardization, and maintain historical data using SCD Type 2 patterns.\n",
        "\n",
        "Once data is cleansed and validated, I move to the Gold Layer, where I aggregate and apply business logic — such as Cost Per Kilometer calculations, contract profitability scoring,\n",
        "and excess mileage projections — storing curated data for reporting, analytics, and downstream consumption.\n",
        "I build batch pipelines using Azure Data Factory, perform complex transformations using PySpark and Spark SQL, and have implemented a metadata-driven validation framework for data quality and governance.\n",
        "\n",
        "One of my key achievements was being part of the migration of the TCO business line from Azure to Microsoft Fabric — — this involved re-architecting pipelines to leverage OneLake for unified storage,\n",
        "Data Pipelines for orchestration, and Fabric Notebooks for PySpark transformations, resulting in improved performance and simplified governance.\n",
        "\n",
        "My core tech stack includes: Azure Data Factory, Databricks, Synapse Analytics, ADLS Gen2, Delta Lake, PySpark, Spark SQL, and Microsoft Fabric (Lakehouse, Notebooks, Data Pipelines, and Power BI with Direct Lake)\n",
        "\n",
        "I have strong experience in building scalable, production-grade data solutions in both Azure and Fabric."
      ],
      "metadata": {
        "id": "zjqnc7HC7vr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End To End Pipeline**"
      ],
      "metadata": {
        "id": "_GQcWHO48Mrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "============================================================\n",
        "ADF SQL Framework Flow (Full + Incremental + Backfill + History)\n",
        "============================================================\n",
        "\"\"\" Pre-requist -\n",
        "01.ON-prim connection onprim-gate way/SHIR\n",
        "02.Metadata tbl -> to define what to load and How to load\n",
        "03.Control tbl -> for tracking when and status of ingestion\n",
        "\"\"\"\n",
        "\n",
        "1) Lookup Activity (Config Table)\n",
        "   Use: Get list of tables + load type (FULL/INCR/BACKFILL)\n",
        "\n",
        "2) ForEach Activity (Iterate Tables)\n",
        "   Use: Loop through each table one by one\n",
        "\n",
        "3) If Condition Activity (Full Load Check)\n",
        "   Use: Decide whether table needs initial history load\n",
        "   - If FULL → Run full extract\n",
        "   - Else → Go incremental path\n",
        "\n",
        "4) Lookup Activity (Watermark Table)\n",
        "   Use: Fetch last processed value for current table\n",
        "\n",
        "5) If Condition Activity (Backfill Check)\n",
        "   Use: Decide whether to reprocess last N days or only new delta\n",
        "\n",
        "6) Copy Activity (SQL → ADLS Bronze)\n",
        "   Use: Load data from SQL source into raw zone\n",
        "   - FULL: All rows\n",
        "   - INCR: Only rows > watermark\n",
        "   - BACKFILL: Rows from last N days\n",
        "\n",
        "7) Notebook / Stored Procedure Activity (Merge History)\n",
        "   - to update the water mark column and full the audit table\n",
        "\n",
        "8) Web activity\n",
        "-for send mail allert using logic app\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jvUm5Vd28T4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schema Drift**"
      ],
      "metadata": {
        "id": "tOhrizJE67Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Schema Drift:\n",
        "\n",
        "\"In our Azure Data Engineering platform for ALD Automotive TCO analytics,\n",
        "schema drift was a common challenge because upstream systems like leasing,\n",
        "maintenance, and fuel providers frequently introduced changes such as new\n",
        "columns, missing fields, or data type modifications. These unexpected schema\n",
        "changes often caused ingestion failures or inconsistencies in downstream\n",
        "reporting.\n",
        "\n",
        "To handle this, I implemented a controlled schema management approach.\n",
        "In the Bronze layer, we allowed flexible raw ingestion so that files could\n",
        "land successfully even if minor schema changes occurred. However, before\n",
        "moving data into Silver and Gold layers, we enforced strict schema validation\n",
        "using an approved schema registry and metadata checks.\n",
        "\n",
        "If an unexpected schema change was detected, the pipeline flagged the issue,\n",
        "logged it for review, and prevented corrupted data from impacting curated\n",
        "tables.\n",
        "\n",
        "This approach significantly reduced pipeline failures, improved data quality,\n",
        "and ensured that TCO reporting remained consistent and reliable even as\n",
        "source systems evolved.\"\n"
      ],
      "metadata": {
        "id": "ATmRFMl666y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Late Arrival Files**"
      ],
      "metadata": {
        "id": "wx21R7XD6hJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"In our Azure Data Engineering platform for ALD Automotive TCO analytics,\n",
        "late arriving files were a frequent challenge because upstream systems like\n",
        "leasing, maintenance, and fuel providers did not always deliver data on time.\n",
        "For example, our ingestion pipelines were scheduled at 2 AM, but some daily\n",
        "vehicle cost files would arrive several hours later or even the next day.\n",
        "\n",
        "To handle this, I implemented a watermark-based incremental ingestion\n",
        "framework in Azure Data Factory. We maintained a control table in Azure SQL\n",
        "to store the last successfully processed business date. Each pipeline run\n",
        "first performed a Lookup to fetch the watermark, then dynamically processed\n",
        "any unprocessed dates instead of only relying on the current day’s schedule.\n",
        "This ensured late files were automatically picked up in the next run without\n",
        "manual reruns.\n",
        "\n",
        "Additionally, I added file validation using Get Metadata and If Condition,\n",
        "so missing files were logged and alerts were triggered instead of breaking\n",
        "downstream jobs.\n",
        "\n",
        "This approach improved data completeness, reduced operational overhead, and\n",
        "kept TCO reporting accurate and reliable.\"\n",
        "\n",
        "To solve this, I implemented a watermark-based incremental ingestion\n",
        "framework in Azure Data Factory.\n",
        "\n",
        "A (Action):\n",
        "We maintained a control table in Azure SQL that stored the last successfully\n",
        "processed business date for each pipeline. At the start of every run, the\n",
        "pipeline performed a Lookup activity to fetch the watermark date, and then\n",
        "dynamically identified which dates were still pending ingestion.\n",
        "\n",
        "Instead of processing only the current day’s file, the pipeline was designed\n",
        "to automatically pick up any unprocessed files whenever they arrived.\n",
        "\n",
        "Additionally, I added file existence validation using Get Metadata and\n",
        "If Condition activities. If the expected file was missing, the pipeline\n",
        "did not fail abruptly. Instead, it logged the missing file event and\n",
        "triggered alerts, preventing downstream jobs from running on incomplete data.\"\n",
        "\n",
        "R (Result):\n",
        "\"As a result, late arriving files were handled automatically without manual\n",
        "reruns, data completeness improved significantly, operational effort reduced,\n",
        "and the business received accurate and consistent TCO reporting even when\n",
        "source deliveries were delayed."
      ],
      "metadata": {
        "id": "yFZjUhcs6gx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SDC Type-2 Implemnetataion using Delta merge**"
      ],
      "metadata": {
        "id": "4XUq4jLf_f7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MERGE INTO dim_customer t\n",
        "USING source_customer s\n",
        "ON t.customer_id = s.customer_id   #business keys\n",
        "AND t.is_current = true\n",
        "\n",
        "WHEN MATCHED AND t.city <> s.city #attributes not matched means chnages\n",
        "THEN UPDATE SET\n",
        "  t.end_date = current_date(),\n",
        "  t.is_current = false\n",
        "\n",
        "WHEN MATCHED AND t.city <> s.city\n",
        "THEN INSERT (\n",
        "  customer_id, city, start_date, end_date, is_current\n",
        ")\n",
        "VALUES (\n",
        "  s.customer_id, s.city, current_date(), NULL, true\n",
        ")\n",
        "\n",
        "WHEN NOT MATCHED\n",
        "THEN INSERT (\n",
        "  customer_id, city, start_date, end_date, is_current\n",
        ")\n",
        "VALUES (\n",
        "  s.customer_id, s.city, current_date(), NULL, true\n",
        ");\n",
        "\n"
      ],
      "metadata": {
        "id": "iddiOrun_llz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Row Level Security (RLS)**"
      ],
      "metadata": {
        "id": "s6bFu5Ux-I5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWn3Y8aj-IDy"
      },
      "outputs": [],
      "source": [
        "#Row Filter Function Using Group Membership\n",
        "\n",
        "#step :1\n",
        "CREATE FUNCTION main_catalog.company.group_rls(dept STRING)\n",
        "RETURN\n",
        "  CASE\n",
        "    WHEN is_member('hr_team') AND dept = 'HR' THEN TRUE\n",
        "    WHEN is_member('finance_team') AND dept = 'Finance' THEN TRUE\n",
        "    ELSE FALSE\n",
        "  END;\n",
        "#step :2\n",
        "ALTER TABLE main_catalog.company.employee\n",
        "SET ROW FILTER main_catalog.company.group_rls ON (dept);\n",
        "\n",
        "\n",
        "#Row Filter Function Using Current User\n",
        "#step :1\n",
        "CREATE FUNCTION main_catalog.company.user_rls(dept STRING)\n",
        "RETURN\n",
        "  CASE\n",
        "    WHEN current_user() = 'ravi@datacorp.com' AND dept = 'HR'\n",
        "    THEN TRUE\n",
        "    ELSE FALSE\n",
        "  END;\n",
        "\n",
        "#Step :2\n",
        "ALTER TABLE main_catalog.company.employee\n",
        "SET ROW FILTER main_catalog.company.user_rls ON (dept);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Column Level Security (CLS)**"
      ],
      "metadata": {
        "id": "Q-qy6wZG--y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GROUP LEVEL\n",
        "#Step :1\n",
        "CREATE FUNCTION main_catalog.company.group_mask_salary(salary INT)\n",
        "RETURN\n",
        "  CASE\n",
        "    WHEN is_member('finance_team')\n",
        "    THEN salary\n",
        "    ELSE NULL\n",
        "  END;\n",
        "\n",
        "#Step :2\n",
        "ALTER TABLE employee\n",
        "ALTER COLUMN salary\n",
        "SET MASK main_catalog.company.group_mask_salary;\n",
        "\n",
        "#User Level\n",
        "#Step :1\n",
        "CREATE FUNCTION main_catalog.company.user_mask_salary(salary INT)\n",
        "RETURN\n",
        "  CASE\n",
        "    WHEN current_user() = 'john@datacorp.com'\n",
        "    THEN salary\n",
        "    ELSE NULL\n",
        "  END;\n",
        "\n",
        "#Step :2\n",
        "ALTER TABLE employee\n",
        "ALTER COLUMN salary\n",
        "SET MASK main_catalog.company.user_mask_salary;\n"
      ],
      "metadata": {
        "id": "-7Y9TMny_FZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- SIMPLE AND CORRECT SCD TYPE-2 IN DELTA (2 STEPS)\n",
        "\n",
        "-- ===============================\n",
        "-- STEP 1: Expire old record\n",
        "-- ===============================\n",
        "MERGE INTO dim_customer t\n",
        "USING source_customer s\n",
        "ON t.customer_id = s.customer_id\n",
        "AND t.is_current = true\n",
        "\n",
        "WHEN MATCHED AND t.city <> s.city\n",
        "THEN UPDATE SET\n",
        "  t.end_date = current_date(),\n",
        "  t.is_current = false;\n",
        "\n",
        "\n",
        "-- ===============================\n",
        "-- STEP 2: Insert new record\n",
        "-- (new customer OR changed customer)\n",
        "-- ===============================\n",
        "INSERT INTO dim_customer\n",
        "SELECT\n",
        "  s.customer_id,\n",
        "  s.city,\n",
        "  current_date() AS start_date,\n",
        "  NULL           AS end_date,\n",
        "  true           AS is_current\n",
        "FROM source_customer s\n",
        "LEFT JOIN dim_customer t\n",
        "  ON s.customer_id = t.customer_id\n",
        " AND t.is_current = true\n",
        "WHERE t.customer_id IS NULL\n",
        "   OR t.city <> s.city;\n",
        "\n",
        "-- Done: History stored correctly\n"
      ],
      "metadata": {
        "id": "OiIHoDYwbjzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADF- Azure datafactory"
      ],
      "metadata": {
        "id": "AbBWUIbESDP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tracking Failures and re-try**"
      ],
      "metadata": {
        "id": "kvApxI_iSLSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"In our Azure Data Engineering platform, I tracked ingestion failures using\n",
        "a centralized audit logging framework. Every ADF pipeline run wrote details\n",
        "into an audit table, including the source name, file or table processed,\n",
        "run status, rows loaded, and error message. This gave complete visibility\n",
        "into which ingestion failed and why.\n",
        "\n",
        "At the activity level, I used ADF’s On Failure paths to capture error\n",
        "information and log failures immediately. For transient issues like network\n",
        "timeouts or temporary source unavailability, I configured retry policies in\n",
        "Copy activities, typically with 2–3 retries and defined retry intervals.\n",
        "\n",
        "If failures persisted beyond the retry limit, the pipeline triggered alerts\n",
        "through email or Teams notifications with run details, so support teams could\n",
        "act quickly.\n",
        "\n",
        "This approach ensured reliable ingestion, faster issue resolution, and\n",
        "minimal data pipeline downtime."
      ],
      "metadata": {
        "id": "ap_zokyDSRDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**how you will pass parameter in runtime**"
      ],
      "metadata": {
        "id": "9ERAmf0EWIOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"In ADF, I pass runtime parameters by defining pipeline parameters like\n",
        "table name or file name. The master pipeline reads these values from a\n",
        "config table using a Lookup activity and loops through them with ForEach.\n",
        "Then it passes the values to Copy or child pipelines, so the same pipeline\n",
        "can load different tables or files without creating separate pipelines.\""
      ],
      "metadata": {
        "id": "NR8_CONMWQKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End to end pipeline with files**"
      ],
      "metadata": {
        "id": "3qrhco8HOVpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sure. Below is a real-world enterprise design for SFTP/ADLS file ingestion\n",
        "with multiple file formats, date-wise folders, and a config-driven framework\n",
        "(ALD Automotive TCO style).\n",
        "\n",
        "============================================================\n",
        "1. Real-World SFTP Folder Structure (Date-wise Delivery)\n",
        "============================================================\n",
        "\n",
        "Upstream systems drop daily files like this:\n",
        "\n",
        "SFTP Root: /ald_tco/\n",
        "\n",
        "------------------------------------------------------------\n",
        "/ald_tco/vehicle_cost/2026/01/29/\n",
        "    vehicle_cost_20260129.csv\n",
        "\n",
        "/ald_tco/fuel_transactions/2026/01/29/\n",
        "    fuel_txn_20260129.json\n",
        "\n",
        "/ald_tco/maintenance_invoice/2026/01/29/\n",
        "    maint_inv_20260129.parquet\n",
        "------------------------------------------------------------\n",
        "\n",
        "3 Different File Formats:\n",
        "- CSV  (vehicle cost)\n",
        "- JSON (fuel transactions)\n",
        "- Parquet (maintenance invoices)\n",
        "\n",
        "============================================================\n",
        "2. Config Table (Drives All File Pipelines)\n",
        "============================================================\n",
        "\n",
        "Table Name: file_ingestion_config\n",
        "\n",
        "---------------------------------------------------------------------------------------------------\n",
        "source_name     source_folder              file_pattern              file_type   target_table\n",
        "---------------------------------------------------------------------------------------------------\n",
        "vehicle_cost    /ald_tco/vehicle_cost/     vehicle_cost_*.csv        CSV         silver_vehicle_cost\n",
        "fuel_txn        /ald_tco/fuel_transactions/ fuel_txn_*.json          JSON        silver_fuel_txn\n",
        "maint_invoice   /ald_tco/maintenance_invoice/ maint_inv_*.parquet   PARQUET     silver_maintenance\n",
        "---------------------------------------------------------------------------------------------------\n",
        "\n",
        "Additional Columns (Enterprise):\n",
        "\n",
        "---------------------------------------------------------------------------------------------------\n",
        "date_partitioned   expected_daily   validation_required\n",
        "---------------------------------------------------------------------------------------------------\n",
        "Y                  Y                Y\n",
        "Y                  Y                Y\n",
        "Y                  Y                N\n",
        "---------------------------------------------------------------------------------------------------\n",
        "\n",
        "============================================================\n",
        "3. Processed File Log Table (Avoid Duplicates + Late Files)\n",
        "============================================================\n",
        "\n",
        "Table: processed_file_log\n",
        "\n",
        "---------------------------------------------------------------------------------------------------\n",
        "source_name      file_name                    processed_date     status\n",
        "---------------------------------------------------------------------------------------------------\n",
        "vehicle_cost     vehicle_cost_20260128.csv    2026-01-28         SUCCESS\n",
        "fuel_txn         fuel_txn_20260128.json       2026-01-28         SUCCESS\n",
        "---------------------------------------------------------------------------------------------------\n",
        "\n",
        "============================================================\n",
        "4. Parameterized ADF Pipeline Flow (Real World)\n",
        "============================================================\n",
        "\n",
        "Pipeline Name: PL_File_Master_Ingestion\n",
        "\n",
        "Pipeline Parameter:\n",
        "- p_source_name (optional, else process all)\n",
        "\n",
        "------------------------------------------------------------\n",
        "Activity Flow\n",
        "------------------------------------------------------------\n",
        "\n",
        "1) Lookup Activity (Read Config)\n",
        "   Use: Get all file sources to ingest today\n",
        "\n",
        "   Query:\n",
        "   SELECT * FROM file_ingestion_config\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "2) ForEach Activity (Loop Each Source)\n",
        "   Use: Iterate vehicle_cost, fuel_txn, maint_invoice\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "3) Set Variable (Build Today Folder Path)\n",
        "   Use: Construct dynamic date-wise path\n",
        "\n",
        "   Example Expression:\n",
        "   @concat(\n",
        "     item().source_folder,\n",
        "     formatDateTime(utcNow(),'yyyy/MM/dd'),\n",
        "     '/'\n",
        "   )\n",
        "\n",
        "   Output:\n",
        "   /ald_tco/vehicle_cost/2026/01/29/\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "4) Get Metadata Activity (List Files)\n",
        "   Use: List files in today’s folder\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "5) Lookup Activity (Processed File Log)\n",
        "   Use: Fetch already processed files for this source\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "6) Filter Activity (New/Late Files Only)\n",
        "   Use: Remove files already processed\n",
        "        Keep only unprocessed arrivals\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "7) ForEach Activity (Loop Each File)\n",
        "\n",
        "   Inside:\n",
        "\n",
        "   7a) Copy Activity (SFTP → Bronze ADLS)\n",
        "       Use: Land raw file into Bronze zone\n",
        "\n",
        "       Target Path:\n",
        "       /bronze/@{item().source_name}/\n",
        "\n",
        "   7b) Notebook Activity (Format-Specific Load)\n",
        "       Use: Read based on file_type\n",
        "\n",
        "       If CSV → spark.read.csv()\n",
        "       If JSON → spark.read.json()\n",
        "       If PARQUET → spark.read.parquet()\n",
        "\n",
        "       Write into Silver table:\n",
        "       silver_vehicle_cost, silver_fuel_txn, etc.\n",
        "\n",
        "   7c) Log File Processed\n",
        "       Use: Insert into processed_file_log\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "8) If Condition (Missing File Alert)\n",
        "   Use: If expected_daily = Y and no file found → send alert\n",
        "\n",
        "        |\n",
        "        v\n",
        "\n",
        "[End]\n",
        "\n",
        "============================================================\n",
        "5. How Late Arriving Files Are Handled\n",
        "============================================================\n",
        "\n",
        "Example:\n",
        "vehicle_cost_20260129.csv arrives late on 30th Jan.\n",
        "\n",
        "Next pipeline run:\n",
        "- Get Metadata lists file\n",
        "- Filter checks processed_file_log\n",
        "- File not processed → picked automatically\n",
        "\n",
        "No manual rerun needed.\n",
        "\n",
        "============================================================\n",
        "One Line Summary\n",
        "============================================================\n",
        "\n",
        "Config-driven source list → Build date folder → List files →\n",
        "Filter unprocessed → Copy to Bronze → Load to Silver → Log processed →\n",
        "Alert if missing\n",
        "\n",
        "============================================================\n",
        "\n",
        "If you want, I can give the exact Filter expression logic used in ADF\n",
        "to compare Get Metadata output with processed_file_log.\n"
      ],
      "metadata": {
        "id": "2EOyB-AF3yXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}